# Model configuration
model:
  name: "google/gemma-3-27b-it"      # Gemma 3 27B-IT (matches available SAEs)
  fallback: "meta-llama/Llama-3.1-8B-Instruct"  # if insufficient VRAM
  dtype: "bfloat16"
  device_map: "auto"

# Assistant Axis
axis:
  # Extracted vectors for Gemma 3 27B-IT (run scripts/extract_axis.py first)
  source: "data/axis_vectors/gemma_3_27b_it_axis.pt"
  # Fallback: pre-computed for Gemma 2 (wrong dimensions for Gemma 3)
  # source: "lu-christina/assistant-axis-vectors"
  target_layer: 31  # mid-to-late layer; must match an available SAE layer

# SAE configuration
sae:
  # Gemma Scope 2 for Gemma 3 27B-IT (instruction-tuned, residual stream)
  release: "gemma-scope-2-27b-it-res"
  sae_id: "layer_31_width_65k_l0_medium"  # explicit SAE ID
  layer: 31                           # available: 16, 31, 40, 53
  width: 65536                        # 65K dict; options: 16K, 65K, 262K, 1M
  top_k: 15                           # features to include in report
  # Neuronpedia ID for descriptions
  neuronpedia_id: "gemma-3-27b-it/31-gemmascope-2-res-65k"

# Meditation parameters
meditation:
  drift_threshold_percentile: 25      # trigger meditation below this percentile
  cooldown_turns: 3                   # minimum turns between meditations
  injection_strategy: "system_pre"    # one of: system_pre, tool_response, thinking_prefix
  always_pulse_check: true            # lightweight axis projection every turn

# Calibration
calibration:
  n_conversations: 500                # number of normal convos for calibration
  dataset: "lmsys/lmsys-chat-1m"     # source of calibration prompts
  output_path: "data/calibration/normal_range.pt"

# Evaluation
eval:
  judge_model: "claude-sonnet-4-20250514"  # for harmfulness classification
  n_multi_turn_convos: 50             # per domain for drift eval
  max_turns: 20                       # per conversation
