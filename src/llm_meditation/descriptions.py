"""
Feature description lookup from Neuronpedia.

Each SAE feature has an auto-interp explanation generated by Neuronpedia.
We fetch these descriptions and cache them locally for fast lookup during
meditation report generation.

API docs: https://github.com/hijohnnylin/neuronpedia
"""

import json
import logging
import time
from pathlib import Path

import requests

from llm_meditation.utils import get_project_root

logger = logging.getLogger("llm_meditation")

NEURONPEDIA_API_BASE = "https://www.neuronpedia.org/api"
API_DELAY_SECONDS = 0.1  # 100ms between API calls


def fetch_description(
    model_id: str, sae_id: str, feature_index: int
) -> str:
    """
    Fetch a human-readable description for a single SAE feature from Neuronpedia.

    Args:
        model_id: Neuronpedia model ID (e.g. "gemma-2-27b")
        sae_id: SAE identifier on Neuronpedia
        feature_index: Feature index in the SAE dictionary

    Returns:
        Description string, or "Unknown feature {index}" on failure.
    """
    url = f"{NEURONPEDIA_API_BASE}/feature/{model_id}/{sae_id}/{feature_index}"
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()

        # Extract the top explanation
        explanations = data.get("explanations", [])
        if explanations:
            desc = explanations[0].get("description", "")
            if desc:
                return desc

        # Fallback: try other fields
        if "description" in data and data["description"]:
            return data["description"]

        return f"Unknown feature {feature_index}"

    except requests.exceptions.RequestException as e:
        logger.warning(f"Failed to fetch description for feature {feature_index}: {e}")
        return f"Unknown feature {feature_index}"
    except (KeyError, IndexError, json.JSONDecodeError) as e:
        logger.warning(f"Failed to parse description for feature {feature_index}: {e}")
        return f"Unknown feature {feature_index}"


class DescriptionCache:
    """
    Persistent JSON-based cache for Neuronpedia feature descriptions.

    Stores descriptions locally to avoid repeated API calls.
    """

    def __init__(self, model_id: str, sae_id: str, cache_dir: Path | None = None):
        self.model_id = model_id
        self.sae_id = sae_id

        if cache_dir is None:
            cache_dir = get_project_root() / "data" / "feature_cache"
        cache_dir.mkdir(parents=True, exist_ok=True)

        # Sanitize filename
        safe_name = f"{model_id}_{sae_id}".replace("/", "_").replace("\\", "_")
        self.cache_path = cache_dir / f"{safe_name}.json"
        self._cache: dict[str, str] = {}
        self.load()

    def load(self) -> None:
        """Load cache from disk."""
        if self.cache_path.exists():
            try:
                with open(self.cache_path) as f:
                    self._cache = json.load(f)
                logger.info(
                    f"Loaded {len(self._cache)} cached descriptions from {self.cache_path}"
                )
            except (json.JSONDecodeError, IOError) as e:
                logger.warning(f"Could not load cache from {self.cache_path}: {e}")
                self._cache = {}
        else:
            self._cache = {}

    def save(self) -> None:
        """Persist cache to disk."""
        with open(self.cache_path, "w") as f:
            json.dump(self._cache, f, indent=2)
        logger.debug(f"Saved {len(self._cache)} descriptions to {self.cache_path}")

    def get(self, feature_index: int) -> str | None:
        """Look up a cached description. Returns None if not cached."""
        return self._cache.get(str(feature_index))

    def set(self, feature_index: int, description: str) -> None:
        """Add a description to the cache and persist."""
        self._cache[str(feature_index)] = description
        self.save()

    def get_or_fetch(self, feature_index: int) -> str:
        """Get from cache, or fetch from API and cache the result."""
        cached = self.get(feature_index)
        if cached is not None:
            return cached

        desc = fetch_description(self.model_id, self.sae_id, feature_index)
        self.set(feature_index, desc)
        time.sleep(API_DELAY_SECONDS)
        return desc

    def prefetch(self, feature_indices: list[int]) -> None:
        """
        Batch fetch and cache descriptions for missing features.

        Respects rate limits with delays between API calls.
        """
        missing = [idx for idx in feature_indices if self.get(idx) is None]
        if not missing:
            logger.info("All requested features already cached")
            return

        logger.info(f"Prefetching {len(missing)} feature descriptions...")
        for i, idx in enumerate(missing):
            desc = fetch_description(self.model_id, self.sae_id, idx)
            self._cache[str(idx)] = desc
            if (i + 1) % 50 == 0:
                logger.info(f"  Fetched {i + 1}/{len(missing)}")
                self.save()  # Checkpoint
            time.sleep(API_DELAY_SECONDS)

        self.save()
        logger.info(f"Prefetched {len(missing)} descriptions (total cached: {len(self._cache)})")

    def __len__(self) -> int:
        return len(self._cache)

    def __contains__(self, feature_index: int) -> bool:
        return str(feature_index) in self._cache


def prefetch_top_features(
    model_id: str,
    sae_id: str,
    sae,
    prompts: list[str] | None = None,
    n: int = 1000,
    cache_dir: Path | None = None,
) -> DescriptionCache:
    """
    Pre-cache descriptions for the N most commonly activated features.

    Runs the SAE encoder on a batch of diverse prompts, collects all
    feature activations, finds the top-N most frequently active features,
    then prefetches their descriptions.

    Args:
        model_id: Neuronpedia model ID
        sae_id: Neuronpedia SAE ID
        sae: Loaded SAE model (SAELens)
        prompts: Optional list of prompt activation tensors. If None, uses random inputs.
        n: Number of top features to prefetch
        cache_dir: Override cache directory

    Returns:
        Populated DescriptionCache
    """
    import torch

    cache = DescriptionCache(model_id, sae_id, cache_dir)

    # If we don't have real activations, use random to discover features
    # In practice, you'd pass real activations from diverse prompts
    if prompts is None:
        logger.info("No activations provided; using random inputs to discover features")
        d_in = sae.cfg.d_in
        random_acts = torch.randn(100, d_in, dtype=sae.dtype, device=sae.device)
    else:
        random_acts = prompts

    # Run through SAE encoder and count feature activations
    with torch.no_grad():
        feature_acts = sae.encode(random_acts)  # (batch, d_sae)
        # Count how often each feature is active (> 0)
        active_counts = (feature_acts > 0).sum(dim=0)  # (d_sae,)

    # Get top-N most frequently active features
    top_n = min(n, active_counts.shape[0])
    _, top_indices = torch.topk(active_counts, top_n)
    top_indices = top_indices.tolist()

    logger.info(f"Found {top_n} most frequently active features, prefetching descriptions...")
    cache.prefetch(top_indices)

    return cache
